<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/icon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/icon.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zhuzhuzai.top","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.25.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="Tensorflow lite 处理模型，运行模型和委托的过程。">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow lite源码分析(Tensorflow 2.14)">
<meta property="og:url" content="https://zhuzhuzai.top/2023/09/22/note/Tensorflow-lite%20source%20code%20analysis/index.html">
<meta property="og:site_name" content="Zzzai&#39;s Blog">
<meta property="og:description" content="Tensorflow lite 处理模型，运行模型和委托的过程。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-09-21T16:00:00.000Z">
<meta property="article:modified_time" content="2025-10-07T11:22:14.529Z">
<meta property="article:author" content="Zhuzhuzai">
<meta property="article:tag" content="gpu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://zhuzhuzai.top/2023/09/22/note/Tensorflow-lite%20source%20code%20analysis/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://zhuzhuzai.top/2023/09/22/note/Tensorflow-lite%20source%20code%20analysis/","path":"2023/09/22/note/Tensorflow-lite source code analysis/","title":"Tensorflow lite源码分析(Tensorflow 2.14)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Tensorflow lite源码分析(Tensorflow 2.14) | Zzzai's Blog</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Zzzai's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorflow-lite%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-Tensorflow-2-14"><span class="nav-number">1.</span> <span class="nav-text">Tensorflow lite源码分析(Tensorflow 2.14)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A3%85%E8%BD%BD%E7%8E%B0%E6%9C%89%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.1.</span> <span class="nav-text">装载现有模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.</span> <span class="nav-text">处理模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.</span> <span class="nav-text">运行模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A7%94%E6%89%98"><span class="nav-number">1.4.</span> <span class="nav-text">委托</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">1.5.</span> <span class="nav-text">参考链接</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhuzhuzai"
      src="/images/portrait.gif">
  <p class="site-author-name" itemprop="name">Zhuzhuzai</p>
  <div class="site-description" itemprop="description">Talk is cheap, show me the code!</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/unsatisfying" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;unsatisfying" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:824941284@qq.com" title="E-Mail → mailto:824941284@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhuzhuzai.top/2023/09/22/note/Tensorflow-lite%20source%20code%20analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/portrait.gif">
      <meta itemprop="name" content="Zhuzhuzai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zzzai's Blog">
      <meta itemprop="description" content="Talk is cheap, show me the code!">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Tensorflow lite源码分析(Tensorflow 2.14) | Zzzai's Blog">
      <meta itemprop="description" content="Tensorflow lite 处理模型，运行模型和委托的过程。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tensorflow lite源码分析(Tensorflow 2.14)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-09-22 00:00:00" itemprop="dateCreated datePublished" datetime="2023-09-22T00:00:00+08:00">2023-09-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-10-07 19:22:14" itemprop="dateModified" datetime="2025-10-07T19:22:14+08:00">2025-10-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/note/" itemprop="url" rel="index"><span itemprop="name">note</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">Tensorflow lite 处理模型，运行模型和委托的过程。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="Tensorflow-lite源码分析-Tensorflow-2-14"><a href="#Tensorflow-lite源码分析-Tensorflow-2-14" class="headerlink" title="Tensorflow lite源码分析(Tensorflow 2.14)"></a>Tensorflow lite源码分析(Tensorflow 2.14)</h1><h2 id="装载现有模型"><a href="#装载现有模型" class="headerlink" title="装载现有模型"></a>装载现有模型</h2><p>​	首先Android app使用tensorflow api中的interpreter创建了一个解释器，该解释器有多种构造函数和继承，这里使用的是文件中读取模型，同时使用了一个委托(delegate)，但是我们最后再来看这个委托是什么。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> val tflite by lazy &#123;</span><br><span class="line">       Interpreter(</span><br><span class="line">           FileUtil.loadMappedFile(<span class="built_in">this</span>, MODEL_PATH),</span><br><span class="line">           Interpreter.Options().addDelegate(nnApiDelegate))</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">private</span> val detector by lazy &#123;</span><br><span class="line">       ObjectDetectionHelper(</span><br><span class="line">           tflite,</span><br><span class="line">           FileUtil.loadLabels(<span class="built_in">this</span>, LABELS_PATH)</span><br><span class="line">       )</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>​	该函数调用了<code>Interpreter</code>类，具体函数过程如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java</span></span><br><span class="line">NativeInterpreterWrapper(ByteBuffer buffer, InterpreterImpl.Options options) &#123;</span><br><span class="line">    TensorFlowLite.init();</span><br><span class="line">	......</span><br><span class="line">    <span class="built_in">this</span>.modelByteBuffer = buffer;</span><br><span class="line">    <span class="type">long</span> <span class="variable">errorHandle</span> <span class="operator">=</span> createErrorReporter(ERROR_BUFFER_SIZE);</span><br><span class="line">    <span class="type">long</span> <span class="variable">modelHandle</span> <span class="operator">=</span> createModelWithBuffer(modelByteBuffer, errorHandle);</span><br><span class="line">    init(errorHandle, modelHandle, options);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>​	核心在于第四行和第六行，第四行将buffer赋给this对象，第六行通过buffer创建模型。这里就接着调用了JNI跳转到C&#x2F;C++的实现。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc</span></span><br><span class="line"><span class="function">JNIEXPORT jlong JNICALL</span></span><br><span class="line"><span class="function"><span class="title">Java_org_tensorflow_lite_NativeInterpreterWrapper_createModelWithBuffer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    JNIEnv* env, jclass <span class="comment">/*clazz*/</span>, jobject model_buffer, jlong error_handle)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (!tflite::jni::<span class="built_in">CheckJniInitializedOrThrow</span>(env)) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  BufferErrorReporter* error_reporter =</span><br><span class="line">      <span class="built_in">convertLongToErrorReporter</span>(env, error_handle);</span><br><span class="line">  <span class="keyword">if</span> (error_reporter == <span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  <span class="type">const</span> <span class="type">char</span>* buf =</span><br><span class="line">      <span class="built_in">static_cast</span>&lt;<span class="type">char</span>*&gt;(env-&gt;<span class="built_in">GetDirectBufferAddress</span>(model_buffer));</span><br><span class="line">  jlong capacity = env-&gt;<span class="built_in">GetDirectBufferCapacity</span>(model_buffer);</span><br><span class="line">  <span class="keyword">if</span> (!<span class="built_in">VerifyModel</span>(buf, capacity)) &#123;</span><br><span class="line">    <span class="built_in">ThrowException</span>(</span><br><span class="line">        env, tflite::jni::kIllegalArgumentException,</span><br><span class="line">        <span class="string">&quot;ByteBuffer is not a valid TensorFlow Lite model flatbuffer&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">auto</span> model = FlatBufferModel::<span class="built_in">BuildFromBuffer</span>(</span><br><span class="line">      buf, <span class="built_in">static_cast</span>&lt;<span class="type">size_t</span>&gt;(capacity), error_reporter);</span><br><span class="line">  <span class="keyword">if</span> (!model) &#123;</span><br><span class="line">    <span class="built_in">ThrowException</span>(env, tflite::jni::kIllegalArgumentException,</span><br><span class="line">                   <span class="string">&quot;ByteBuffer does not encode a valid model: %s&quot;</span>,</span><br><span class="line">                   error_reporter-&gt;<span class="built_in">CachedErrorMessage</span>());</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">reinterpret_cast</span>&lt;jlong&gt;(model.<span class="built_in">release</span>());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​	在上面逻辑中，最核心的一行是第18行，从buffer创建模型。其调用的是<code>model_builder.cc</code>文件下的<code>BuildFromBuffer</code>函数.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//tensorflow/lite/core/model_builder.cc</span></span><br><span class="line"><span class="function">std::unique_ptr&lt;FlatBufferModel&gt; <span class="title">FlatBufferModel::BuildFromBuffer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">char</span>* caller_owned_buffer, <span class="type">size_t</span> buffer_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    ErrorReporter* error_reporter)</span> </span>&#123;</span><br><span class="line"><span class="comment">//调用ValidateErrorReporter函数，检查error_reporter是否为nullptr，如果是，则返回默认的错误报告器，否则返回原来的error_reporter。</span></span><br><span class="line">  error_reporter = <span class="built_in">ValidateErrorReporter</span>(error_reporter);</span><br><span class="line"><span class="comment">//创建一个MemoryAllocation对象，它是Allocation类的子类，用于封装缓冲区的地址和大小，并提供读取和写入的接口。MemoryAllocation对象使用new操作符在堆上分配内存，并使用caller_owned_buffer, buffer_size, error_reporter作为构造函数的参数。</span></span><br><span class="line">  <span class="function">std::unique_ptr&lt;Allocation&gt; <span class="title">allocation</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">new</span> MemoryAllocation(caller_owned_buffer, buffer_size, error_reporter))</span></span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">BuildFromAllocation</span>(std::<span class="built_in">move</span>(allocation), error_reporter);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">ErrorReporter* <span class="title">ValidateErrorReporter</span><span class="params">(ErrorReporter* e)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> e ? e : <span class="built_in">DefaultErrorReporter</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MemoryAllocation::<span class="built_in">MemoryAllocation</span>(<span class="type">const</span> <span class="type">void</span>* ptr, <span class="type">size_t</span> num_bytes, ErrorReporter* error_reporter)</span><br><span class="line">: <span class="built_in">Allocation</span>(error_reporter, Allocation::Type::kMemory) &#123;</span><br><span class="line">	......</span><br><span class="line">  buffer_ = ptr;</span><br><span class="line">  buffer_size_bytes_ = num_bytes;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​	在做了一些错误检查和给buffer分配内存后转化成专门的Allocation类(这么做的原因还是为了抽象，把各种从文件读的模型，从内存buffer读的模型都转化成allocation类，最后进行统一处理)，然后进行核心的函数处理<code>BuildFromAllocation</code>:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tensorflow/lite/core/model_builder.cc</span></span><br><span class="line"><span class="function">std::unique_ptr&lt;FlatBufferModel&gt; <span class="title">FlatBufferModel::BuildFromAllocation</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    std::unique_ptr&lt;Allocation&gt; allocation, ErrorReporter* error_reporter)</span> </span>&#123;</span><br><span class="line">  <span class="function">std::unique_ptr&lt;FlatBufferModel&gt; <span class="title">model</span><span class="params">(<span class="keyword">new</span> FlatBufferModel(</span></span></span><br><span class="line"><span class="params"><span class="function">      std::move(allocation), ValidateErrorReporter(error_reporter)))</span></span>;</span><br><span class="line">  <span class="keyword">if</span> (!model-&gt;<span class="built_in">initialized</span>()) &#123;</span><br><span class="line">    model.<span class="built_in">reset</span>();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    model-&gt;<span class="built_in">ValidateModelBuffers</span>(error_reporter);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> model;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// FlatBufferModel的构造函数实现</span></span><br><span class="line"><span class="comment">// 将model直接传递给类型model_成员变量</span></span><br><span class="line">FlatBufferModel::<span class="built_in">FlatBufferModel</span>(<span class="type">const</span> Model* model, ErrorReporter* error_reporter)</span><br><span class="line">    : <span class="built_in">model_</span>(model), <span class="built_in">error_reporter_</span>(<span class="built_in">ValidateErrorReporter</span>(error_reporter)) &#123;&#125;</span><br><span class="line"><span class="comment">//将allocation变量直接传递给allocation_变量，然后使用GetModel对allocation进行处理。</span></span><br><span class="line">FlatBufferModel::<span class="built_in">FlatBufferModel</span>(std::unique_ptr&lt;Allocation&gt; allocation, ErrorReporter* error_reporter)</span><br><span class="line">    : <span class="built_in">error_reporter_</span>(<span class="built_in">ValidateErrorReporter</span>(error_reporter)), <span class="built_in">allocation_</span>(std::<span class="built_in">move</span>(allocation)) &#123;</span><br><span class="line">  <span class="keyword">if</span> (!allocation_ || !allocation_-&gt;<span class="built_in">valid</span>() || !<span class="built_in">CheckModelIdentifier</span>()) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  model_ = ::tflite::<span class="built_in">GetModel</span>(allocation_-&gt;<span class="built_in">base</span>());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​	它使用了<code>GetModel</code>方法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tensorflow/tensorflow/lite/core/model_builder.h</span></span><br><span class="line"><span class="function"><span class="type">const</span> tflite::Model* <span class="title">GetModel</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> model_; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// tensorflow/tensorflow/lite/schema/schema_generated.h</span></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">const</span> tflite::Model *<span class="title">GetModel</span><span class="params">(<span class="type">const</span> <span class="type">void</span> *buf)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> ::flatbuffers::<span class="built_in">GetRoot</span>&lt;tflite::Model&gt;(buf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​	可以发现这里调用了google的<code>flatbuffer</code>库，具体源码参考<a target="_blank" rel="noopener" href="https://github.com/google/flatbuffers/blob/d3e8cb60a133be5387008864d3fc212e31774b63/include/flatbuffers/buffer.h">flatbuffers&#x2F;include&#x2F;flatbuffers&#x2F;buffer.h at d3e8cb60a133be5387008864d3fc212e31774b63 · google&#x2F;flatbuffers (github.com)</a>。实际调用引入过程是<code>schema_generated.h</code> 代码中引入了头文件 <code>#include &quot;flatbuffers/flatbuffers.h&quot;</code> ，然后 <code>flatbuffers/flatbuffers.h</code> 引入头文件<code>flatbuffers/buffer.h</code>,最终得到<code>GetModel</code>的实现。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/google/flatbuffers/blob/d3e8cb60a133be5387008864d3fc212e31774b63/include/flatbuffers/buffer.h</span></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt; <span class="type">const</span> T *<span class="title">GetRoot</span><span class="params">(<span class="type">const</span> <span class="type">void</span> *buf)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">GetMutableRoot</span>&lt;T&gt;(<span class="built_in">const_cast</span>&lt;<span class="type">void</span> *&gt;(buf));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt; T *<span class="title">GetMutableRoot</span><span class="params">(<span class="type">void</span> *buf)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (!buf) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  <span class="built_in">EndianCheck</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">reinterpret_cast</span>&lt;T *&gt;(</span><br><span class="line">      <span class="built_in">reinterpret_cast</span>&lt;<span class="type">uint8_t</span> *&gt;(buf) +</span><br><span class="line">      <span class="built_in">EndianScalar</span>(*<span class="built_in">reinterpret_cast</span>&lt;<span class="type">uoffset_t</span> *&gt;(buf)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​	OK 到这一步为止，我们可以知道实际仅仅是吧flatbuffer格式的文件读进了内存，并用各种抽象进行管理，内存布局仍然是flatbuffer中定义的内存布局。并且最后的包装成了一个Model对象，<code>Model *_model</code>。</p>
<p>然后让我们来看看TFLite中模型具体是怎么定义的吧，这个涉及到的文件为<code>schema.fbs(tensorflow/tensorflow/lite/schema/schema.fbs)</code></p>
<ul>
<li><p>Model:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">table Model &#123;</span><br><span class="line">  <span class="comment">// 一个int32类型的值，表示模型的版本号。目前最新的版本号是3</span></span><br><span class="line">  version:uint;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 一个OperatorCode类型的数组，表示模型中使用的算子的编码。每个算子有一个唯一的编码，用于标识算子的名称和版本。</span></span><br><span class="line">  operator_codes:[OperatorCode];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 一个SubGraph类型的数组，表示模型中包含的子图。每个子图有一个输入张量列表、一个输出张量列表、一个状态张量列表、一个算子节点列表和一个名称。子图是模型执行的基本单元，可以理解为一个计算图. 0th子图为main图</span></span><br><span class="line">  subgraphs:[SubGraph];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// A description of the model.</span></span><br><span class="line">  description:string;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 一个Buffer类型的数组，表示模型中所有张量数据的存储空间。每个缓冲区有一个字节数组和一个哈希值。通过缓冲区的索引，可以从字节数组中读取或写入张量的数据。0th的buffer必须是空buffer</span></span><br><span class="line">  <span class="comment">// Note the 0th entry of this array must be an empty buffer (sentinel).</span></span><br><span class="line">  <span class="comment">// This is a convention so that tensors without a buffer can provide 0 as</span></span><br><span class="line">  <span class="comment">// their buffer.</span></span><br><span class="line">  buffers:[Buffer];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 一个int32类型的数组，表示模型中包含的元数据所在的缓冲区的索引。元数据是一种用于描述模型属性和关联文件等信息的结构，可以用于提高模型的可解释性和兼容性。</span></span><br><span class="line">  metadata_buffer:[<span class="type">int</span>];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Metadata about the model. 包含name和buffer，即名字字段和该metadata所在的buffer</span></span><br><span class="line">  metadata:[Metadata];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 一个SignatureDef类型的数组，表示模型中包含的签名定义。每个签名定义有一个输入映射、一个输出映射和一个方法名称。签名定义是一种用于描述模型输入和输出接口以及功能等信息的结构，可以用于提高模型的可用性和灵活性。</span></span><br><span class="line">  signature_defs:[SignatureDef];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Subgraph:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">table SubGraph &#123;</span><br><span class="line">  <span class="comment">// A list of all tensors used in this subgraph.</span></span><br><span class="line">  tensors:[Tensor];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 子图的输入张量索引，例如inputs [0] 表示tensors数组中第0个张量表示输入张量</span></span><br><span class="line">  inputs:[<span class="type">int</span>];</span><br><span class="line">  <span class="comment">// 输出张量索引</span></span><br><span class="line">  outputs:[<span class="type">int</span>];</span><br><span class="line">  <span class="comment">// operators数组</span></span><br><span class="line">  operators:[Operator];</span><br><span class="line">  name:string;</span><br><span class="line">&#125;</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
</li>
<li><p>Tensor: </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">table Tensor &#123;</span><br><span class="line">  <span class="comment">// int 类型的数字，表示张量的形状，例如[1, 224, 224, 3],表示1个图片样本，224*224像素大小，RGB3个通道</span></span><br><span class="line">  shape:[<span class="type">int</span>];</span><br><span class="line">  <span class="comment">// 张量的数据类型，比如int32， float32， int8等等 ，也定义在该文件中的enum TensorType字段</span></span><br><span class="line">  type:TensorType;</span><br><span class="line">  <span class="comment">// buffer：一个int32类型的值，表示张量的数据所在的缓冲区的索引。缓冲区是一种存储模型中所有张量数据的结构，每个缓冲区有一个唯一的索引和一个字节数组。通过这个字段，可以从缓冲区中读取或写入张量的数据。</span></span><br><span class="line">  buffer:uint;</span><br><span class="line">  name:string;  <span class="comment">// 一个string类型的值，表示张量的名称。这个字段可以用于标识或描述张量的作用或来源，也可以用于调试或可视化。</span></span><br><span class="line">  quantization:QuantizationParameters;  <span class="comment">// Optional. 一个QuantizationParameters类型的表，表示张量的量化参数。量化是一种将浮点数转换为整数或低位数来减少模型大小和提高性能的技术。这个表中包含了一些用于反量化或重新量化张量数据的参数，如最小值、最大值、比例因子、零点等。</span></span><br><span class="line"></span><br><span class="line">  is_variable:<span class="type">bool</span> = <span class="literal">false</span>; <span class="comment">//一个bool类型的值，表示张量是否是变量。变量是一种可以在模型运行过程中改变值的张量，通常用于存储模型的参数或状态。如果这个字段为true，则表示张量是变量，否则表示张量是常量。</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 一个SparsityParameters类型的表，表示张量的稀疏性参数。稀疏性是一种将张量中大部分为零的元素压缩或省略来减少模型大小和提高性能的技术。这个表中包含了一些用于解压缩或重新压缩张量数据的参数，如稀疏维度、非零值索引、非零值块等。</span></span><br><span class="line">  sparsity:SparsityParameters;  <span class="comment">// Optional.</span></span><br><span class="line"></span><br><span class="line">  shape_signature:[<span class="type">int</span>]; <span class="comment">// Optional.</span></span><br><span class="line">  has_rank: <span class="type">bool</span> = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 用作嵌套张量类型字段</span></span><br><span class="line">  variant_tensors:[VariantSubType];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Buffer:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">table Buffer &#123;</span><br><span class="line">  data:[ubyte] (force_align: <span class="number">16</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// In a model that is larger than 2GB, then buffers instead uses the following</span></span><br><span class="line">  <span class="comment">// attributes to find stored data, which is outside of flatbuffers</span></span><br><span class="line">  <span class="comment">// the offset is calculated relative to the beginning of the file and is only</span></span><br><span class="line">  <span class="comment">// valid if &gt; 1.</span></span><br><span class="line">  offset: ulong;</span><br><span class="line">  size: ulong;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="处理模型"><a href="#处理模型" class="headerlink" title="处理模型"></a>处理模型</h2><p>​	接着上面的步骤，我们回到java包装器的地方，回顾一下代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java</span></span><br><span class="line">NativeInterpreterWrapper(ByteBuffer buffer, InterpreterImpl.Options options) &#123;</span><br><span class="line">    TensorFlowLite.init();</span><br><span class="line">	......</span><br><span class="line">    <span class="built_in">this</span>.modelByteBuffer = buffer;</span><br><span class="line">    <span class="type">long</span> <span class="variable">errorHandle</span> <span class="operator">=</span> createErrorReporter(ERROR_BUFFER_SIZE);</span><br><span class="line">    <span class="type">long</span> <span class="variable">modelHandle</span> <span class="operator">=</span> createModelWithBuffer(modelByteBuffer, errorHandle);</span><br><span class="line">    init(errorHandle, modelHandle, options);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>​	这里将模型读入buffer后，接着运行了init函数，那么init函数里面干了什么事情呢？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tensorflow/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// errorHandle：一个long类型的值，表示一个错误处理器的句柄，用于报告解释器运行过程中的错误信息。</span></span><br><span class="line"><span class="comment">// modelHandle：一个long类型的值，表示一个模型的句柄，用于加载和访问TensorFlow Lite模型的数据和元数据。</span></span><br><span class="line"><span class="comment">// options：一个InterpreterImpl.Options类型的对象，表示一些解释器的选项，如线程数、加速配置、精度控制等</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">(<span class="type">long</span> errorHandle, <span class="type">long</span> modelHandle, InterpreterImpl.Options options)</span> &#123;</span><br><span class="line">	<span class="comment">// some error handle, 检查options是否为空，检查options中是否有加速配置</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">        </span><br><span class="line">    <span class="built_in">this</span>.errorHandle = errorHandle;</span><br><span class="line">    <span class="built_in">this</span>.modelHandle = modelHandle;</span><br><span class="line">    <span class="comment">// First create the interpreter without delegates.  We need an interpreter in order to figure</span></span><br><span class="line">    <span class="comment">// out whether the model contains any unresolved flex ops, and creating the interpreter with</span></span><br><span class="line">    <span class="comment">// delegates might fail if there are any unresolved flex ops.</span></span><br><span class="line">    <span class="comment">// (Alternatively, we could determine this without needing to recreate the interpreter</span></span><br><span class="line">    <span class="comment">// by passing the tflite::Model in to here, and then traversing that?)</span></span><br><span class="line">    <span class="comment">// 上面这段英文罗里吧嗦一大堆，实际就是先创建一个基本的解释器，如果后续有代理再进行处理。</span></span><br><span class="line">    ArrayList&lt;Long&gt; delegateHandles = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    <span class="built_in">this</span>.interpreterHandle =</span><br><span class="line">        createInterpreter(</span><br><span class="line">            modelHandle,</span><br><span class="line">            errorHandle,</span><br><span class="line">            options.getNumThreads(),</span><br><span class="line">            options.getUseXNNPACK(),</span><br><span class="line">            delegateHandles);</span><br><span class="line">    <span class="comment">// 判断是否有上述interpreter没法处理的op，也就是说上面这个函数就是处理模型成op的真实过程</span></span><br><span class="line">    <span class="built_in">this</span>.originalGraphHasUnresolvedFlexOp = hasUnresolvedFlexOp(interpreterHandle);</span><br><span class="line">    <span class="comment">// 这里添加委托，委托就是一种可以提高模型执行效率和兼容性的机制，可以将模型中的一些操作交给特定的硬件或软件来执行。</span></span><br><span class="line">    addDelegates(options);</span><br><span class="line">    <span class="comment">// 初始化当前对象中包含InterpreterFactory接口的代理。InterpreterFactory接口是一种可以让代理自己创建解释器并管理其生命周期的机制。</span></span><br><span class="line">    initDelegatesWithInterpreterFactory();</span><br><span class="line">    delegateHandles.ensureCapacity(delegates.size());</span><br><span class="line">    <span class="keyword">for</span> (Delegate delegate : delegates) &#123;</span><br><span class="line">      delegateHandles.add(delegate.getNativeHandle());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 说明有代理需要处理，所以把之前创建的基本解释器删除，添加有代理的解释器</span></span><br><span class="line">    <span class="keyword">if</span> (!delegateHandles.isEmpty()) &#123;</span><br><span class="line">      <span class="comment">// If there are any delegates enabled, recreate the interpreter with those delegates.</span></span><br><span class="line">      delete(<span class="comment">/* errorHandle= */</span> <span class="number">0</span>, <span class="comment">/* modelHandle= */</span> <span class="number">0</span>, <span class="built_in">this</span>.interpreterHandle);</span><br><span class="line">      <span class="built_in">this</span>.interpreterHandle =</span><br><span class="line">          createInterpreter(</span><br><span class="line">              modelHandle,</span><br><span class="line">              errorHandle,</span><br><span class="line">              options.getNumThreads(),</span><br><span class="line">              options.getUseXNNPACK(),</span><br><span class="line">              delegateHandles);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 创建两个TensorImpl类型的数组，分别赋值给当前对象的inputTensors和outputTensors成员变量。这两个数组用于存储模型的输入和输出张量的对象。数组的大小分别由调用getInputCount和getOutputCount方法得到。</span></span><br><span class="line">    <span class="built_in">this</span>.inputTensors = <span class="keyword">new</span> <span class="title class_">TensorImpl</span>[getInputCount(interpreterHandle)];</span><br><span class="line">    <span class="built_in">this</span>.outputTensors = <span class="keyword">new</span> <span class="title class_">TensorImpl</span>[getOutputCount(interpreterHandle)];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">    <span class="comment">// 为模型的输入和输出张量分配内存</span></span><br><span class="line">    allocateTensors(interpreterHandle, errorHandle);</span><br><span class="line">    <span class="built_in">this</span>.isMemoryAllocated = <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>​	很复杂，但是我们关注一个核心函数，<code>createInterpreter</code>。同样这里是一个JNI包装器，实际从java代码调用到了C++的API。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tensorflow/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc</span></span><br><span class="line"><span class="function">JNIEXPORT jlong JNICALL</span></span><br><span class="line"><span class="function"><span class="title">Java_org_tensorflow_lite_NativeInterpreterWrapper_createInterpreter</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    JNIEnv* env, jclass clazz, jlong model_handle, jlong error_handle,</span></span></span><br><span class="line"><span class="params"><span class="function">    jint num_threads, jboolean useXnnpack, jobject delegate_handle_list)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// some handler check for JNI</span></span><br><span class="line">  <span class="comment">// 获取两个调用的参数，应该是JNI调用过来的时候需要进行一个数据结构的转换。</span></span><br><span class="line">  FlatBufferModel* model = <span class="built_in">convertLongToModel</span>(env, model_handle);</span><br><span class="line">  <span class="keyword">if</span> (model == <span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  BufferErrorReporter* error_reporter =</span><br><span class="line">      <span class="built_in">convertLongToErrorReporter</span>(env, error_handle);</span><br><span class="line">  <span class="keyword">if</span> (error_reporter == <span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  std::unique_ptr&lt;OpResolver&gt; resolver =</span><br><span class="line">      std::<span class="built_in">make_unique</span>&lt;tflite::jni::OpResolverLazyDelegateProxy&gt;(</span><br><span class="line">          tflite::<span class="built_in">CreateOpResolver</span>(), useXnnpack != JNI_FALSE);</span><br><span class="line"></span><br><span class="line">  <span class="function">InterpreterBuilder <span class="title">interpreter_builder</span><span class="params">(*model, *resolver)</span></span>;</span><br><span class="line">  interpreter_builder.<span class="built_in">SetNumThreads</span>(<span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(num_threads));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Add delegate_list to interpreter_builder.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Java: int size = delegate_list.size();</span></span><br><span class="line">  jint size = env-&gt;<span class="built_in">CallIntMethod</span>(delegate_handle_list, list_size_method);</span><br><span class="line">  <span class="keyword">for</span> (jint i = <span class="number">0</span>; i &lt; size; ++i) &#123;</span><br><span class="line">    <span class="comment">// Java: Long jdelegate_handle = delegate_handle_list-&gt;get(i);</span></span><br><span class="line">    jobject jdelegate_handle =</span><br><span class="line">        env-&gt;<span class="built_in">CallObjectMethod</span>(delegate_handle_list, list_get_method, i);</span><br><span class="line">    <span class="keyword">if</span> (jdelegate_handle == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!env-&gt;<span class="built_in">ExceptionCheck</span>()) &#123;</span><br><span class="line">        <span class="built_in">ThrowException</span>(env, tflite::jni::kIllegalArgumentException,</span><br><span class="line">                       <span class="string">&quot;Internal error: null object in Delegate handle list&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Java: long delegate_handle = jdelegate_handle.longValue();</span></span><br><span class="line">    jlong delegate_handle =</span><br><span class="line">        env-&gt;<span class="built_in">CallLongMethod</span>(jdelegate_handle, long_value_method);</span><br><span class="line">    <span class="keyword">if</span> (delegate_handle == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!env-&gt;<span class="built_in">ExceptionCheck</span>()) &#123;</span><br><span class="line">        <span class="built_in">ThrowException</span>(env, tflite::jni::kIllegalArgumentException,</span><br><span class="line">                       <span class="string">&quot;Internal error: Found invalid handle&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">auto</span> delegate = <span class="built_in">reinterpret_cast</span>&lt;TfLiteOpaqueDelegate*&gt;(delegate_handle);</span><br><span class="line">    interpreter_builder.<span class="built_in">AddDelegate</span>(delegate);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>看来核心就在第15行开始的函数。这一行是什么意思呢，把一些修饰符删去后可以知道，<code>resolver = OpResolverLazyDelegateProxy(CreateOpResolver(), useXnnpack != JNI_FALSE) </code> 所以我们先看看 <code>CreateOpResolver</code> 干了什么事情。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tensorflow/tensorflow/lite/create_op_resolver_with_selected_ops.cc</span></span><br><span class="line"><span class="function">std::unique_ptr&lt;MutableOpResolver&gt; <span class="title">CreateOpResolver</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  std::unique_ptr&lt;MutableOpResolver&gt; resolver =</span><br><span class="line">      std::<span class="built_in">make_unique</span>&lt;MutableOpResolver&gt;();</span><br><span class="line">  <span class="built_in">RegisterSelectedOps</span>(resolver.<span class="built_in">get</span>());</span><br><span class="line">  <span class="keyword">return</span> resolver;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// tensorflow/tensorflow/lite/core/create_op_resolver_with_builtin_ops.cc</span></span><br><span class="line"><span class="function">std::unique_ptr&lt;MutableOpResolver&gt; <span class="title">CreateOpResolver</span><span class="params">()</span> </span>&#123;  <span class="comment">// NOLINT</span></span><br><span class="line">  <span class="keyword">return</span> std::<span class="built_in">unique_ptr</span>&lt;tflite::ops::builtin::BuiltinOpResolver&gt;(</span><br><span class="line">      <span class="keyword">new</span> tflite::ops::builtin::<span class="built_in">BuiltinOpResolverWithoutDefaultDelegates</span>());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BuiltinOpResolver::<span class="built_in">BuiltinOpResolver</span>() &#123;</span><br><span class="line">  <span class="built_in">AddBuiltin</span>(BuiltinOperator_ABS, <span class="built_in">Register_ABS</span>(), <span class="comment">/* min_version = */</span> <span class="number">1</span>,</span><br><span class="line">             <span class="comment">/* max_version = */</span> <span class="number">5</span>);</span><br><span class="line">  <span class="built_in">AddBuiltin</span>(BuiltinOperator_HARD_SWISH, <span class="built_in">Register_HARD_SWISH</span>());</span><br><span class="line">  <span class="built_in">AddBuiltin</span>(BuiltinOperator_RELU, <span class="built_in">Register_RELU</span>(), <span class="comment">/* min_version = */</span> <span class="number">1</span>,</span><br><span class="line">             <span class="comment">/* max_version = */</span> <span class="number">3</span>);</span><br><span class="line">  <span class="built_in">AddBuiltin</span>(BuiltinOperator_RELU_N1_TO_1, <span class="built_in">Register_RELU_N1_TO_1</span>());</span><br><span class="line">  <span class="built_in">AddBuiltin</span>(BuiltinOperator_RELU_0_TO_1, <span class="built_in">Register_RELU_0_TO_1</span>());</span><br><span class="line">  <span class="built_in">AddBuiltin</span>(BuiltinOperator_RELU6, <span class="built_in">Register_RELU6</span>(), <span class="comment">/* min_version = */</span> <span class="number">1</span>,</span><br><span class="line">             <span class="comment">/* max_version = */</span> <span class="number">3</span>);</span><br><span class="line">  <span class="built_in">AddBuiltin</span>(BuiltinOperator_TANH, <span class="built_in">Register_TANH</span>(), <span class="comment">/* min_version = */</span> <span class="number">1</span>,</span><br><span class="line">             <span class="comment">/* max_version = */</span> <span class="number">3</span>);</span><br><span class="line">  <span class="built_in">AddBuiltin</span>(BuiltinOperator_LOGISTIC, <span class="built_in">Register_LOGISTIC</span>(),</span><br><span class="line">             <span class="comment">/* min_version = */</span> <span class="number">1</span>,</span><br><span class="line">             <span class="comment">/* max_version = */</span> <span class="number">3</span>);</span><br><span class="line">  <span class="built_in">AddBuiltin</span>(BuiltinOperator_AVERAGE_POOL_2D, <span class="built_in">Register_AVERAGE_POOL_2D</span>(),</span><br><span class="line">             <span class="comment">/* min_version */</span> <span class="number">1</span>,</span><br><span class="line">             <span class="comment">/* max_version */</span> <span class="number">3</span>);</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"> <span class="keyword">enum</span> <span class="title class_">BuiltinOperator</span> : <span class="type">int32_t</span> &#123;</span><br><span class="line">  BuiltinOperator_ADD = <span class="number">0</span>,</span><br><span class="line">  BuiltinOperator_AVERAGE_POOL_2D = <span class="number">1</span>,</span><br><span class="line">  BuiltinOperator_CONCATENATION = <span class="number">2</span>,</span><br><span class="line">  BuiltinOperator_CONV_2D = <span class="number">3</span>,</span><br><span class="line">  BuiltinOperator_DEPTHWISE_CONV_2D = <span class="number">4</span>,</span><br><span class="line">  BuiltinOperator_DEPTH_TO_SPACE = <span class="number">5</span>,</span><br><span class="line">  BuiltinOperator_DEQUANTIZE = <span class="number">6</span>,</span><br><span class="line">  BuiltinOperator_EMBEDDING_LOOKUP = <span class="number">7</span>,</span><br><span class="line">  BuiltinOperator_FLOOR = <span class="number">8</span>,</span><br><span class="line">     ...</span><br><span class="line">  BuiltinOperator_ABS = <span class="number">101</span>,</span><br><span class="line">     ...</span><br><span class="line">         </span><br><span class="line"><span class="comment">//tensorflow/tensorflow/lite/kernels/elementwise.cc   </span></span><br><span class="line"><span class="comment">// elementwise::ElementWiseQuantizedInit：这个函数用于初始化一个逐元素运算的算子，它会根据输入和输出张量的类型和量化参数，创建一个逐元素运算的上下文对象，并返回其指针。</span></span><br><span class="line"><span class="comment">// elementwise::ElementWiseQuantizedFree：这个函数用于释放一个逐元素运算的算子，它会删除之前创建的逐元素运算的上下文对象，并释放其内存空间。</span></span><br><span class="line"><span class="comment">// PrepareAbs：这个函数用于准备一个绝对值算子，它会检查输入和输出张量的形状是否一致，并根据输入和输出张量的类型和量化参数，计算出绝对值运算所需的乘法和偏移量，并保存在逐元素运算的上下文对象中。</span></span><br><span class="line"><span class="comment">// elementwise::AbsEval：这个函数用于执行一个绝对值算子，它会根据输入和输出张量的类型和量化参数，以及之前计算出的乘法和偏移量，对输入张量中的每个元素求其绝对值，并将结果写入输出张量中。</span></span><br><span class="line"><span class="built_in">GENERIC_PREPARE</span>(PrepareAbs, elementwise::IsAbsSupportedType,</span><br><span class="line">                elementwise::kAbsName)</span><br><span class="line">TfLiteRegistration* <span class="built_in">Register_ABS</span>() &#123;</span><br><span class="line">  <span class="type">static</span> TfLiteRegistration r = &#123;elementwise::ElementWiseQuantizedInit,</span><br><span class="line">                                 elementwise::ElementWiseQuantizedFree,</span><br><span class="line">                                 PrepareAbs, elementwise::AbsEval&#125;;</span><br><span class="line">  <span class="keyword">return</span> &amp;r;</span><br><span class="line">&#125;</span><br><span class="line">     </span><br><span class="line"><span class="function">TfLiteStatus <span class="title">AbsEval</span><span class="params">(TfLiteContext* context, TfLiteNode* node)</span> </span>&#123;</span><br><span class="line">  <span class="type">const</span> TfLiteTensor* input = <span class="built_in">GetInput</span>(context, node, <span class="number">0</span>);</span><br><span class="line">  <span class="type">const</span> TfLiteType type = input-&gt;type;</span><br><span class="line">  <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">    <span class="keyword">case</span> kTfLiteFloat32:</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">EvalImpl</span>&lt;<span class="type">float</span>&gt;(context, node, std::abs&lt;<span class="type">float</span>&gt;, type);</span><br><span class="line">    <span class="keyword">case</span> kTfLiteInt8:</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">AbsEvalQuantized</span>&lt;<span class="type">int8_t</span>&gt;(context, node, type);</span><br><span class="line">    <span class="keyword">case</span> kTfLiteInt16:</span><br><span class="line">      <span class="keyword">return</span> input-&gt;quantization.type == kTfLiteNoQuantization</span><br><span class="line">                 ? <span class="built_in">AbsInt16EvalImpl</span>(context, node, type)</span><br><span class="line">                 : <span class="built_in">AbsEvalQuantized</span>&lt;<span class="type">int16_t</span>&gt;(context, node, type);</span><br><span class="line">    <span class="keyword">case</span> kTfLiteInt32:</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">EvalImpl</span>&lt;<span class="type">int32_t</span>&gt;(context, node, std::abs&lt;<span class="type">int32_t</span>&gt;, type);</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">      <span class="built_in">TF_LITE_KERNEL_LOG</span>(context, <span class="string">&quot;Current data type %s is not supported.&quot;</span>,</span><br><span class="line">                         <span class="built_in">TfLiteTypeGetName</span>(type));</span><br><span class="line">      <span class="keyword">return</span> kTfLiteError;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">     </span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">inline</span> TfLiteStatus <span class="title">EvalImpl</span><span class="params">(TfLiteContext* context, TfLiteNode* node,</span></span></span><br><span class="line"><span class="params"><span class="function">                             std::function&lt;T(T)&gt; func,</span></span></span><br><span class="line"><span class="params"><span class="function">                             std::function&lt;TfLiteStatus(T)&gt; validate_input_func,</span></span></span><br><span class="line"><span class="params"><span class="function">                             TfLiteType expected_type)</span> </span>&#123;</span><br><span class="line">  <span class="type">const</span> TfLiteTensor* input;</span><br><span class="line">  <span class="built_in">TF_LITE_ENSURE_OK</span>(context, <span class="built_in">GetInputSafe</span>(context, node, <span class="number">0</span>, &amp;input));</span><br><span class="line">  TfLiteTensor* output;</span><br><span class="line">  <span class="built_in">TF_LITE_ENSURE_OK</span>(context, <span class="built_in">GetOutputSafe</span>(context, node, <span class="number">0</span>, &amp;output));</span><br><span class="line">  <span class="built_in">TF_LITE_ENSURE_TYPES_EQ</span>(context, input-&gt;type, expected_type);</span><br><span class="line">  <span class="type">const</span> <span class="type">int64_t</span> num_elements = <span class="built_in">NumElements</span>(input);</span><br><span class="line">  <span class="type">const</span> T* in_data = <span class="built_in">GetTensorData</span>&lt;T&gt;(input);</span><br><span class="line">  T* out_data = <span class="built_in">GetTensorData</span>&lt;T&gt;(output);</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int64_t</span> i = <span class="number">0</span>; i &lt; num_elements; ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span> (validate_input_func) &#123;</span><br><span class="line">      <span class="built_in">TF_LITE_ENSURE_OK</span>(context, <span class="built_in">validate_input_func</span>(in_data[i]));</span><br><span class="line">    &#125;</span><br><span class="line">    out_data[i] = <span class="built_in">func</span>(in_data[i]);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> kTfLiteOk;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​	可以发现注册了很多内置的算子。这些算子应该是tensorflow中实现的。实现都在tensorflow&#x2F;tensorflow&#x2F;lite&#x2F;kernels文件夹中。然后实际核心运算可以看到，使用的是std标准库中的<code>std::abs&lt;int32_t&gt;</code>, 这里就能够和内核驱动进行一个连接。</p>
<p>然后再回头来看看<code>InterpreterBuilder interpreter_builder(*model, *resolver);</code> 这个调用了以下构造函数，其中<code>options_experimental</code> 为默认的参数, 也就是进行一些赋值后返回。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tensorflow/tensorflow/lite/core/interpreter_builder.h</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InterpreterBuilder</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">/// For this constructor, the ErrorReporter will be extracted from the</span></span><br><span class="line">  <span class="comment">/// FlatBufferModel.</span></span><br><span class="line">  <span class="comment">/// `options` object is copied during construction. So caller can release it</span></span><br><span class="line">  <span class="comment">// after calling the constructor.</span></span><br><span class="line">  <span class="built_in">InterpreterBuilder</span>(<span class="type">const</span> FlatBufferModel&amp; model,</span><br><span class="line">                     <span class="type">const</span> OpResolver&amp; op_resolver,</span><br><span class="line">                     <span class="type">const</span> InterpreterOptions* options_experimental = <span class="literal">nullptr</span>);</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">// tensorflow/tensorflow/lite/core/interpreter_builder.cc</span></span><br><span class="line">InterpreterBuilder::<span class="built_in">InterpreterBuilder</span>(</span><br><span class="line">    <span class="type">const</span> FlatBufferModel&amp; model, <span class="type">const</span> OpResolver&amp; op_resolver,</span><br><span class="line">    <span class="type">const</span> InterpreterOptions* options_experimental)</span><br><span class="line">    : <span class="built_in">model_</span>(model.<span class="built_in">GetModel</span>()),</span><br><span class="line">      <span class="built_in">op_resolver_</span>(op_resolver),</span><br><span class="line">      <span class="built_in">error_reporter_</span>(<span class="built_in">ValidateErrorReporter</span>(model.<span class="built_in">error_reporter</span>())),</span><br><span class="line">      <span class="built_in">metadata_</span>(model.<span class="built_in">ReadAllMetadata</span>()),</span><br><span class="line">      <span class="built_in">allocation_</span>(model.<span class="built_in">allocation</span>()) &#123;</span><br><span class="line">  <span class="keyword">if</span> (options_experimental) &#123;</span><br><span class="line">    options_ = *options_experimental;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h2><p>​	运行模型首先还是调用的是java的API，这里是run函数。这个函数先判断一些错误检查，然后获取到input tensor，这个可以从之前说过的graph中的index获取。然后如果没有给tensor分配空间则分配空间。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tensorflow/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">run</span><span class="params">(Object[] inputs, Map&lt;Integer, Object&gt; outputs)</span> &#123;</span><br><span class="line">    <span class="comment">// some error check</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// get input tensor and allocate space</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; inputs.length; ++i) &#123;</span><br><span class="line">      <span class="type">TensorImpl</span> <span class="variable">tensor</span> <span class="operator">=</span> getInputTensor(i);</span><br><span class="line">      <span class="type">int</span>[] newShape = tensor.getInputShapeIfDifferent(inputs[i]);</span><br><span class="line">      <span class="keyword">if</span> (newShape != <span class="literal">null</span>) &#123;</span><br><span class="line">        resizeInput(i, newShape);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">boolean</span> <span class="variable">allocatedTensors</span> <span class="operator">=</span> allocateTensorsIfNeeded();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; inputs.length; ++i) &#123;</span><br><span class="line">      getInputTensor(i).setTo(inputs[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">long</span> <span class="variable">inferenceStartNanos</span> <span class="operator">=</span> System.nanoTime();</span><br><span class="line">    run(interpreterHandle, errorHandle);</span><br><span class="line">    <span class="type">long</span> <span class="variable">inferenceDurationNanoseconds</span> <span class="operator">=</span> System.nanoTime() - inferenceStartNanos;</span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment">// tensorflow/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java        </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(<span class="type">long</span> interpreterHandle, <span class="type">long</span> errorHandle)</span>;</span><br></pre></td></tr></table></figure>

<p>最后又调用了重载的run函数，可以看到又是一个JNI跳转，于是再次回到<code>tensorflow/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc</code> 搜索跳转的包装函数run。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">JNIEXPORT <span class="keyword">void</span> JNICALL <span class="title function_">Java_org_tensorflow_lite_NativeInterpreterWrapper_run</span><span class="params">(</span></span><br><span class="line"><span class="params">    JNIEnv* env, jclass clazz, jlong interpreter_handle, jlong error_handle)</span> &#123;</span><br><span class="line">  <span class="comment">// ......</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (interpreter-&gt;Invoke() != kTfLiteOk) &#123;</span><br><span class="line">    <span class="comment">// TODO(b/168266570): Return InterruptedException.</span></span><br><span class="line">    ThrowException(env, tflite::jni::kIllegalArgumentException,</span><br><span class="line">                   <span class="string">&quot;Internal error: Failed to run on the given Interpreter: %s&quot;</span>,</span><br><span class="line">                   error_reporter-&gt;CachedErrorMessage());</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​	核心在于第五行，调用了interpreter结构体的invoke函数。我们看看invoke函数做了什么, 这个interpreter已经是C++的结构体了，不再是java的结构体了，所以直接调用的C++代码，在<code>tensorflow/tensorflow/lite/core/interpreter.cc</code>文件。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tensorflow/tensorflow/lite/core/interpreter.cc</span></span><br><span class="line"><span class="function">TfLiteStatus <span class="title">Interpreter::Invoke</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="function">ScopedRuntimeInstrumentationProfile <span class="title">scoped_runtime_event</span><span class="params">(root_profiler_.get(),</span></span></span><br><span class="line"><span class="params"><span class="function">                                                           <span class="string">&quot;invoke&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ......</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">TF_LITE_ENSURE_STATUS_WITH_SCOPED_INSTRUMENTATION</span>(</span><br><span class="line">      scoped_runtime_event, <span class="built_in">primary_subgraph</span>().<span class="built_in">Invoke</span>());</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!allow_buffer_handle_output_) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> tensor_index : <span class="built_in">outputs</span>()) &#123;</span><br><span class="line">      <span class="built_in">TF_LITE_ENSURE_STATUS_WITH_SCOPED_INSTRUMENTATION</span>(</span><br><span class="line">          scoped_runtime_event,</span><br><span class="line">          <span class="built_in">primary_subgraph</span>().<span class="built_in">EnsureTensorDataIsReadable</span>(tensor_index));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> kTfLiteOk;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​	可以发现调用了<code>primary_subgraph().Invoke()</code>,我们先看看<code>primary_subgraph()</code> 是什么东西:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">//tensorflow/tensorflow/lite/core/interpreter.h</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Interpreter</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:  </span><br><span class="line">    <span class="function">Subgraph&amp; <span class="title">primary_subgraph</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> *subgraphs_.<span class="built_in">front</span>();  <span class="comment">// Safe as subgraphs_ always has 1 entry.</span></span><br><span class="line">      &#125;</span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="comment">// Subgraphs</span></span><br><span class="line">  std::vector&lt;std::unique_ptr&lt;Subgraph&gt;&gt; subgraphs_;</span><br><span class="line">   <span class="comment">// ......</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//tensorflow/tensorflow/lite/core/interpreter.cc</span></span><br><span class="line">    Interpreter::<span class="built_in">Interpreter</span>(ErrorReporter* error_reporter)</span><br><span class="line">        : <span class="built_in">error_reporter_</span>(error_reporter ? error_reporter</span><br><span class="line">                                         : <span class="built_in">DefaultErrorReporter</span>()) &#123;</span><br><span class="line">    <span class="comment">//......</span></span><br><span class="line">            <span class="built_in">AddSubgraphs</span>(<span class="number">1</span>);</span><br><span class="line">            ...</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">Interpreter::AddSubgraphs</span><span class="params">(<span class="type">int</span> subgraphs_to_add,</span></span></span><br><span class="line"><span class="params"><span class="function">                                   <span class="type">int</span>* first_new_subgraph_index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">// first_new_subgraph_index 默认是nullptr</span></span><br><span class="line">      <span class="type">const</span> <span class="type">size_t</span> base_index = subgraphs_.<span class="built_in">size</span>();</span><br><span class="line">      <span class="keyword">if</span> (first_new_subgraph_index) *first_new_subgraph_index = base_index;</span><br><span class="line"></span><br><span class="line">      subgraphs_.<span class="built_in">reserve</span>(base_index + subgraphs_to_add);</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; subgraphs_to_add; ++i) &#123;</span><br><span class="line">        Subgraph* subgraph = <span class="keyword">new</span> <span class="built_in">Subgraph</span>(</span><br><span class="line">            error_reporter_, external_contexts_, &amp;subgraphs_, &amp;resources_,</span><br><span class="line">            &amp;resource_ids_, &amp;initialization_status_map_, subgraphs_.<span class="built_in">size</span>());</span><br><span class="line">        subgraphs_.<span class="built_in">emplace_back</span>(subgraph);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> 这里设计到了一个<code>subgraphs_</code>的变量，这是什么呢，这其实是一个子图的vector容器，在创建Interpreter时，其有一个私有的成员变量为<code>subgraphs_</code>, 并且在Interpreter的构造函数中会将index为0的图加入这个vector中，也就是说primary_subgraph就是最初完整的那个模型图，后续会对这个图进行分裂成各个子图。这一部分在table subgraph中已经讲得很明确了。	</p>
<p>​	OK， 知道了就是获取到从文件中读取到的模型图，然后回来<code>invoke</code>函数,又是一层皮，还调用了<code>InvokeImpl</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//tensorflow/tensorflow/lite/core/subgraph.cc</span></span><br><span class="line"><span class="function">TfLiteStatus <span class="title">Subgraph::Invoke</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> status = <span class="built_in">InvokeImpl</span>();</span><br><span class="line">  telemetry::<span class="built_in">TelemetryReportEvent</span>(&amp;context_, <span class="string">&quot;Invoke&quot;</span>, status);</span><br><span class="line">  <span class="keyword">return</span> status;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">TfLiteStatus <span class="title">Subgraph::InvokeImpl</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// ......</span></span><br><span class="line">  </span><br><span class="line">    <span class="built_in">EnsureTensorsVectorCapacity</span>();</span><br><span class="line">    tensor_resized_since_op_invoke_ = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">auto</span> s = <span class="built_in">OpInvoke</span>(registration, &amp;node); s != kTfLiteOk) &#123;</span><br><span class="line">      <span class="keyword">auto</span> err = <span class="built_in">ReportOpError</span>(&amp;context_, node, registration, node_index,</span><br><span class="line">                               <span class="string">&quot;failed to invoke&quot;</span>);</span><br><span class="line">      <span class="keyword">return</span> s == kTfLiteCancelled ? s : err;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">//......</span></span><br><span class="line">  <span class="keyword">return</span> status;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以发现调用了<code>OpInvoke</code>函数, 然后他又调用了Registration的invoke函数，这就和前面结合起来了。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TfLiteStatus <span class="title">Subgraph::OpInvoke</span><span class="params">(<span class="type">const</span> TfLiteRegistration&amp; op_reg,</span></span></span><br><span class="line"><span class="params"><span class="function">                                TfLiteNode* node)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (op_reg.registration_external &amp;&amp;</span><br><span class="line">      op_reg.registration_external-&gt;node_index != <span class="number">-1</span>) &#123;</span><br><span class="line">    TfLiteRegistration* referenced_registration =</span><br><span class="line">        &amp;nodes_and_registration_[op_reg.registration_external-&gt;node_index]</span><br><span class="line">             .second;</span><br><span class="line">    <span class="keyword">if</span> (referenced_registration-&gt;invoke == <span class="literal">nullptr</span>) <span class="keyword">return</span> kTfLiteError;</span><br><span class="line">    <span class="keyword">return</span> referenced_registration-&gt;<span class="built_in">invoke</span>(&amp;context_, node);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (op_reg.registration_external &amp;&amp; op_reg.registration_external-&gt;invoke) &#123;</span><br><span class="line">    <span class="keyword">return</span> op_reg.registration_external-&gt;<span class="built_in">invoke</span>(</span><br><span class="line">        <span class="built_in">reinterpret_cast</span>&lt;TfLiteOpaqueContext*&gt;(&amp;context_),</span><br><span class="line">        <span class="built_in">reinterpret_cast</span>&lt;TfLiteOpaqueNode*&gt;(node));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (op_reg.invoke == <span class="literal">nullptr</span>) <span class="keyword">return</span> kTfLiteError;</span><br><span class="line">  <span class="keyword">return</span> op_reg.<span class="built_in">invoke</span>(&amp;context_, node);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="委托"><a href="#委托" class="headerlink" title="委托"></a>委托</h2><p>在运行模型那部分代码最后出现了一个结构体<code>TfLiteRegistrationExternal</code>, 这个类需要和<code>TfLiteRegistration</code> 一起分析。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a target="_blank" rel="noopener" href="https://blog.actorsfit.com/a?ID=01000-9ed722be-d08c-4d27-a81e-157358f946e1">TensorFlow Lite 源代码分析-模型加载和执行 - actorfit (actorsfit.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av24219725/">TensorFlow Lite 深度解析 - 谷歌中国工程师教学视频_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://resources.linaro.org/en/resource/ZDqBmZ9TWNc3Yi5vQeiqWF">LVC21-113: TensorFlow Lite Delegates on Arm-based Devices | Linaro Resources Hub</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/gpu/" rel="tag"># gpu</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/09/14/note/cuda_and_GPU/" rel="prev" title="cuda和GPU的探索(转自周杨叶)">
                  <i class="fa fa-angle-left"></i> cuda和GPU的探索(转自周杨叶)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/09/22/note/Tensorflow%20C++%20op/" rel="next" title="Tensorflow OP 代码分析">
                  Tensorflow OP 代码分析 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Zhuzhuzai</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/unsatisfying" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>
  

      
        <script async src=https://cdn.jsdelivr.net/npm/hexo-next-mouse-effect@latest/click/fireWorks.js></script>
      

      s
        <script async src=https://cdn.jsdelivr.net/npm/hexo-next-mouse-effect@latest/move/fairyDustCursor.js></script>
      
    
</body>
</html>
